"""é«˜æ•ˆçš„æµå¼åˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿ - æ”¯æŒåƒä¸‡çº§æ•°æ®å¤„ç†

ä¸»è¦ç‰¹æ€§ï¼š
1. æµå¼å¤„ç†ï¼šç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼Œstageé—´æµæ°´çº¿å¹¶è¡Œ
2. å†…å­˜ç®¡ç†ï¼šé˜Ÿåˆ—èƒŒå‹æ§åˆ¶ï¼Œé¿å…OOM
3. å®¹é”™æœºåˆ¶ï¼šæ£€æŸ¥ç‚¹ã€é‡è¯•ã€æ­»ä¿¡é˜Ÿåˆ—
4. åŠ¨æ€è°ƒåº¦ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´
5. ç›‘æ§é›†æˆï¼šå®æ—¶è¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡
"""

import time
import pickle
import asyncio
from typing import Dict, List, Any, Optional, Callable, Set
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from pathlib import Path
from collections import defaultdict
import threading

import ray
from ray.util.queue import Queue, Empty, Full
from loguru import logger

# ä»é…ç½®ç®¡ç†å™¨å¯¼å…¥PipelineConfig
from src.config.manager import PipelineConfig
from src.common import BatchData, SourceItem


# Pipelineæ§åˆ¶ä¿¡å·
END_OF_STREAM = "END_OF_STREAM"

@dataclass
class PipelineSignal:
    """Pipelineæ§åˆ¶ä¿¡å· - ç”¨äºæ˜ç¡®çš„æµç¨‹æ§åˆ¶ï¼Œæ›¿ä»£None"""
    signal_type: str  # END_OF_STREAM, SHUTDOWN, PAUSE
    source: str  # å‘é€è€…æ ‡è¯† (producer, worker_id)
    target_worker_count: int = 1  # ä¸‹æ¸¸éœ€è¦æ¥æ”¶çš„workeræ•°é‡
    timestamp: float = field(default_factory=time.time)
    
    def __repr__(self) -> str:
        return f"PipelineSignal({self.signal_type}, from={self.source}, targets={self.target_worker_count})"


@ray.remote
class TerminationBarrier:
    """ç»ˆæ­¢ä¿¡å·å±éšœ - è§£å†³å¤šWorkeråœºæ™¯ä¸‹çš„ç»ˆæ­¢ç«äº‰é—®é¢˜"""
    
    def __init__(self, 
                 upstream_worker_count: int, 
                 downstream_worker_count: int,
                 output_queue: Queue,
                 stage_name: str = "unknown"):
        self.upstream_worker_count = upstream_worker_count
        self.downstream_worker_count = downstream_worker_count
        self.output_queue = output_queue
        self.stage_name = stage_name
        self.signals_received = 0
        self.finished = False
        
    def signal(self, source: str) -> None:
        """æ¥æ”¶ä¸Šæ¸¸Workerçš„ç»ˆæ­¢ä¿¡å·"""
        if self.finished:
            return
            
        self.signals_received += 1
        if self.signals_received >= self.upstream_worker_count:
            logger.info(f"[BARRIER:{self.stage_name}] All upstream workers finished. Sending {self.downstream_worker_count} END_OF_STREAM signals downstream.")
        else:
            logger.debug(f"[BARRIER:{self.stage_name}] Received signal from {source} ({self.signals_received}/{self.upstream_worker_count})")
            
            # å‘ä¸‹æ¸¸å‘é€æŒ‡å®šæ•°é‡çš„ç»“æŸä¿¡å·
            for i in range(self.downstream_worker_count):
                signal = PipelineSignal(
                    signal_type=END_OF_STREAM,
                    source=f"barrier_{self.stage_name}",
                    target_worker_count=self.downstream_worker_count
                )
                try:
                    self.output_queue.put(signal, block=True, timeout=30)
                except Full:
                    logger.error(f"[BARRIER:{self.stage_name}] Failed to put END_OF_STREAM signal (Queue Full)")
                except Exception as e:
                    logger.error(f"[BARRIER:{self.stage_name}] Error putting signal: {e}")
            
            self.finished = True



class PipelineStage(ABC):
    """Abstract base class for pipeline stages"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
    @abstractmethod
    def process(self, batch: BatchData) -> BatchData:
        """Process a batch of data"""
        pass


@ray.remote
class StreamingDataProducer:
    """æµå¼æ•°æ®ç”Ÿäº§è€… - é¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®"""
    
    def __init__(self, 
                 data_loader_config: Dict[str, Any],
                 batch_size: int = 32,
                 checkpoint_dir: str = "./checkpoints"):
        from src.data.media_indexer import MediaDataLoader
        from src.data.storage import MediaStorageManager
        
        self.data_loader = MediaDataLoader(data_loader_config)
        self.storage_manager = MediaStorageManager(data_loader_config['storage'])
        self.batch_size = batch_size
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤„ç†çŠ¶æ€
        self.processed_file_ids: Set[str] = set()
        self.current_batch_idx = 0
        self.total_produced = 0
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        self._load_checkpoint()
        
    def _load_checkpoint(self) -> None:
        """åŠ è½½ç”Ÿäº§è€…æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / "producer_checkpoint.pkl"
        if checkpoint_file.exists():
            try:
                with open(checkpoint_file, 'rb') as f:
                    checkpoint = pickle.load(f)
                    self.processed_file_ids = checkpoint['processed_file_ids']
                    self.current_batch_idx = checkpoint['current_batch_idx']
                    self.total_produced = checkpoint['total_produced']
                logger.info(f"Loaded producer checkpoint: {len(self.processed_file_ids)} files processed")
            except Exception as e:
                logger.error(f"Failed to load producer checkpoint: {e}")
    
    def _save_checkpoint(self) -> None:
        """ä¿å­˜ç”Ÿäº§è€…æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / "producer_checkpoint.pkl"
        try:
            checkpoint = {
                'processed_file_ids': self.processed_file_ids,
                'current_batch_idx': self.current_batch_idx,
                'total_produced': self.total_produced,
                'timestamp': time.time()
            }
            with open(checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint, f)
        except Exception as e:
            logger.error(f"Failed to save producer checkpoint: {e}")
    
    def load_index(self) -> List[Dict[str, Any]]:
        """Load media index"""
        df = self.data_loader.load_index()
        if df.empty:
            # Build index from storage if not exists
            media_files = self.storage_manager.list_media_files()
            if media_files:
                df = self.data_loader.create_index(media_files)
        return df.to_dict('records')
    
    def stream_batches(self, output_queue: Queue, max_batches: Optional[int] = None,
                       num_downstream_workers: int = 1) -> None:
        """æµå¼äº§ç”Ÿæ•°æ®æ‰¹æ¬¡åˆ°é˜Ÿåˆ—
        
        Args:
            output_queue: è¾“å‡ºé˜Ÿåˆ—
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            num_downstream_workers: ä¸‹æ¸¸stageçš„workeræ•°é‡ï¼Œç”¨äºå‘é€æ­£ç¡®æ•°é‡çš„ç»“æŸä¿¡å·
        """
        try:
            audio_records = self.load_index()
            logger.info(f"[PRODUCER] Total records in index: {len(audio_records)}")
            
            # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
            remaining_records = [
                record for record in audio_records 
                if record['file_id'] not in self.processed_file_ids
            ]
            logger.info(f"[PRODUCER] Remaining records to process: {len(remaining_records)}")
            logger.info(f"[PRODUCER] Will send {num_downstream_workers} END_OF_STREAM signals when done")
            
            batch_count = 0
            checkpoint_interval = 100  # æ¯100ä¸ªbatchä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            
            # æµå¼äº§ç”Ÿæ‰¹æ¬¡
            for i in range(self.current_batch_idx, len(remaining_records), self.batch_size):
                if max_batches and batch_count >= max_batches:
                    break
                
                batch_records = remaining_records[i:i + self.batch_size]
                # Convert records to SourceItem objects
                items = [
                    SourceItem(
                        file_id=r['file_id'],
                        oss_path=r['oss_path'],
                        format=r.get('format', 'wav'),
                        duration=r.get('duration', 0.0),
                        metadata={k: v for k, v in r.items() 
                                 if k not in ['file_id', 'oss_path', 'format', 'duration']}
                    ) for r in batch_records
                ]

                batch = BatchData(
                    batch_id=f"batch_{self.total_produced}",
                    items=items,
                    metadata={'stage': 'producer', 'batch_index': self.total_produced}
                )
                
                logger.debug(f"[PRODUCER] Created batch '{batch.batch_id}' with {len(items)} SourceItems")
                
                # Rate limit to prevent object store flooding
                time.sleep(0.01)
                
                # å°†batchæ”¾å…¥é˜Ÿåˆ—ï¼ˆä¼šé˜»å¡ç›´åˆ°é˜Ÿåˆ—æœ‰ç©ºé—´ï¼‰
                try:
                    output_queue.put(batch, block=True, timeout=60)
                    self.total_produced += 1
                    self.current_batch_idx = i + self.batch_size
                    batch_count += 1
                    
                    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                    if batch_count % checkpoint_interval == 0:
                        self._save_checkpoint()
                        logger.info(f"[PRODUCER] Checkpoint saved: {batch_count} batches produced")
                    
                except Full:
                    logger.warning("[PRODUCER] Output queue full, retrying...")
                    time.sleep(1)
            
            # å‘é€å¤šä¸ªç»“æŸä¿¡å·ï¼ˆæ¯ä¸ªä¸‹æ¸¸workerä¸€ä¸ªï¼‰
            for i in range(num_downstream_workers):
                end_signal = PipelineSignal(
                    signal_type=END_OF_STREAM,
                    source="producer",
                    target_worker_count=num_downstream_workers
                )
                output_queue.put(end_signal, block=True)
                logger.info(f"[PRODUCER] Sent END_OF_STREAM signal {i+1}/{num_downstream_workers}")
            
            # æœ€ç»ˆä¿å­˜æ£€æŸ¥ç‚¹
            self._save_checkpoint()
            
            logger.info(f"[PRODUCER] Completed: {batch_count} batches produced, {num_downstream_workers} end signals sent")
            
        except Exception as e:
            import traceback
            logger.error(f"[PRODUCER] Error: {e}")
            logger.error(f"[PRODUCER] Traceback:\n{traceback.format_exc()}")
            # å‘é€ç»“æŸä¿¡å·ä»¥é¿å…ä¸‹æ¸¸workeræ— é™ç­‰å¾…
            for i in range(num_downstream_workers):
                end_signal = PipelineSignal(
                    signal_type=END_OF_STREAM,
                    source="producer_error",
                    target_worker_count=num_downstream_workers
                )
                output_queue.put(end_signal, block=True)
            raise
    
    def mark_batch_processed(self, file_ids: List[str]) -> None:
        """æ ‡è®°æ‰¹æ¬¡å·²å¤„ç†"""
        self.processed_file_ids.update(file_ids)
        self._save_checkpoint()


@ray.remote
class StreamingPipelineWorker:
    """æµå¼Pipeline Worker - æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥Stage"""
    
    def __init__(self,
                 worker_id: str,
                 stage_name: str,
                 stage_class: type,
                 stage_config: Dict[str, Any],
                 max_retries: int = 3):
        self.worker_id = worker_id
        self.stage_name = stage_name
        self.stage = stage_class(stage_config)
        self.max_retries = max_retries
        
        # æ£€æµ‹æ˜¯å¦ä¸ºå¼‚æ­¥Stage
        self.is_async_stage = hasattr(self.stage, 'process_async')
        
        # å¦‚æœæ˜¯å¼‚æ­¥Stageï¼Œåˆ›å»ºäº‹ä»¶å¾ªç¯
        self.loop = None
        if self.is_async_stage:
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)
            logger.info(f"Worker {self.worker_id} initialized with async event loop")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.processed_count = 0
        self.error_count = 0
        self.total_processing_time = 0.0
    
    def _count_item_types(self, items: List) -> Dict[str, int]:
        """ç»Ÿè®¡itemsä¸­å„ç±»å‹çš„æ•°é‡"""
        from collections import Counter
        type_counts = Counter(type(item).__name__ for item in items)
        return dict(type_counts)
        
    def process_stream(self,
                      input_queue: Queue,
                      output_queue: Queue,
                      dead_letter_queue: Queue,
                      num_downstream_workers: int = 1,
                      barrier_actor: Optional[Any] = None) -> Dict[str, Any]:
        """ä»è¾“å…¥é˜Ÿåˆ—æµå¼å¤„ç†æ•°æ®
        
        Args:
            input_queue: è¾“å…¥é˜Ÿåˆ—
            output_queue: è¾“å‡ºé˜Ÿåˆ—
            dead_letter_queue: æ­»ä¿¡é˜Ÿåˆ—
            num_downstream_workers: ä¸‹æ¸¸stageçš„workeræ•°é‡ï¼Œç”¨äºå‘é€æ­£ç¡®æ•°é‡çš„ç»“æŸä¿¡å·
            barrier_actor: ç»ˆæ­¢å±éšœActor (TerminationBarrier)
        """
        logger.info(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] Started, downstream_workers={num_downstream_workers}")
        
        try:
            while True:
                try:
                    # ä»è¾“å…¥é˜Ÿåˆ—è·å–æ‰¹æ¬¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
                    batch = input_queue.get(block=True, timeout=10)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºPipelineSignalç»“æŸä¿¡å·
                    if isinstance(batch, PipelineSignal):
                        logger.info(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] Received {batch}")
                        
                        if barrier_actor:
                            # ä½¿ç”¨å±éšœåè°ƒç»ˆæ­¢
                            logger.info(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] Signaling termination barrier...")
                            barrier_actor.signal.remote(self.worker_id)
                        else:
                            # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥å‘ä¸‹æ¸¸å‘é€å¯¹åº”æ•°é‡çš„ç»“æŸä¿¡å·
                            for i in range(num_downstream_workers):
                                downstream_signal = PipelineSignal(
                                    signal_type=END_OF_STREAM,
                                    source=self.worker_id,
                                    target_worker_count=num_downstream_workers
                                )
                                output_queue.put(downstream_signal, block=True)
                            logger.info(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] Forwarded {num_downstream_workers} END_OF_STREAM signals to downstream")
                        break
                    
                    # å‘åå…¼å®¹ï¼šå¤„ç† None ä¿¡å·ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
                    if batch is None:
                        logger.warning(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] Received legacy None signal")
                        output_queue.put(None, block=True)
                        break
                    
                    # å¤„ç†æ‰¹æ¬¡
                    start_time = time.time()
                    # è¯¦ç»†æ—¥å¿—: è¾“å…¥batchä¿¡æ¯(DEBUG)
                    item_types = self._count_item_types(batch.items)
                    logger.debug(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] INPUT batch '{batch.batch_id}' | items={len(batch.items)} | types={item_types}")
                    
                    try:
                        # æ ¹æ®Stageç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
                        if self.is_async_stage:
                            # å¼‚æ­¥Stageï¼šåœ¨äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œ
                            result = self.loop.run_until_complete(
                                self.stage.process_async(batch)
                            )
                        else:
                            # åŒæ­¥Stageï¼šç›´æ¥è°ƒç”¨
                            result = self.stage.process(batch)
                        
                        result.metadata['worker_id'] = self.worker_id
                        result.metadata['stage'] = self.stage_name
                        result.metadata['processed_at'] = time.time()
                        # è¯¦ç»†æ—¥å¿—: è¾“å‡ºbatchä¿¡æ¯(DEBUG)
                        output_item_types = self._count_item_types(result.items)
                        processing_time = time.time() - start_time
                        logger.debug(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] OUTPUT batch '{batch.batch_id}' | input={len(batch.items)} -> output={len(result.items)} | types={output_item_types} | time={processing_time:.2f}s")
                        
                        # å¦‚æœitemæ•°é‡å˜åŒ–æ˜æ˜¾ï¼Œé¢å¤–è¾“å‡ºè­¦å‘Š
                        if len(result.items) == 0 and len(batch.items) > 0:
                            logger.warning(f"[STAGE:{self.stage_name}] Batch '{batch.batch_id}' produced ZERO output items from {len(batch.items)} inputs!")
                        elif len(result.items) > len(batch.items) * 10:
                            logger.debug(f"[STAGE:{self.stage_name}] Batch '{batch.batch_id}' EXPANDED: {len(batch.items)} -> {len(result.items)} items (expansion stage)")
                        
                        # æ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
                        # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥åº”å¯¹èƒŒå‹ï¼Œé¿å…ç›´æ¥æŠ¥é”™ (60s -> 300s)
                        try:
                            output_queue.put(result, block=True, timeout=300)
                        except Full:
                            logger.error(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] CRITICAL: Output queue FULL after 300s wait. Deadlock potential!")
                            raise
                        
                        self.processed_count += 1
                        self.total_processing_time += time.time() - start_time
                        
                        # å®šæœŸæ‰“å°çŠ¶æ€ (æ¯100ä¸ªbatch)
                        if self.processed_count % 100 == 0:
                            avg_time = self.total_processing_time / self.processed_count
                            logger.info(f"[STAGE:{self.stage_name}][WORKER:{self.worker_id}] Processed {self.processed_count} batches | Avg time: {avg_time:.3f}s")
                        
                    except Exception as e:
                        import traceback
                        logger.error(f"[STAGE:{self.stage_name}] ERROR processing batch '{batch.batch_id}': {e}")
                        logger.error(f"[STAGE:{self.stage_name}] Traceback:\n{traceback.format_exc()}")
                        
                        # è¾“å‡ºbatchä¸­itemsçš„è¯¦ç»†ä¿¡æ¯ä»¥ä¾¿æ’æŸ¥
                        logger.error(f"[STAGE:{self.stage_name}] Failed batch details: items={len(batch.items)}, types={self._count_item_types(batch.items)}")
                        if batch.items:
                            first_item = batch.items[0]
                            logger.error(f"[STAGE:{self.stage_name}] First item type: {type(first_item).__name__}, has metadata: {hasattr(first_item, 'metadata')}")
                        
                        # é‡è¯•é€»è¾‘
                        batch.retry_count += 1
                        if batch.retry_count <= self.max_retries:
                            logger.warning(f"[STAGE:{self.stage_name}] RETRY batch '{batch.batch_id}' (attempt {batch.retry_count}/{self.max_retries})")
                            input_queue.put(batch, block=True)
                        else:
                            logger.error(f"[STAGE:{self.stage_name}] DEAD LETTER: batch '{batch.batch_id}' failed after {self.max_retries} retries")
                            batch.metadata['error'] = str(e)
                            batch.metadata['error_traceback'] = traceback.format_exc()
                            batch.metadata['failed_worker'] = self.worker_id
                            batch.metadata['failed_stage'] = self.stage_name
                            dead_letter_queue.put(batch, block=True)
                            
                        self.error_count += 1
                        
                except Empty:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                    continue
                except Exception as e:
                    import traceback
                    logger.error(f"Worker {self.worker_id} unexpected error in stage '{self.stage_name}': {e}")
                    logger.error(f"Traceback:\n{traceback.format_exc()}")
                    break
            
            # è¿”å›ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'worker_id': self.worker_id,
                'stage': self.stage_name,
                'processed_count': self.processed_count,
                'error_count': self.error_count,
                'avg_processing_time': (self.total_processing_time / self.processed_count 
                                       if self.processed_count > 0 else 0)
            }
            
            logger.info(f"[STAGE:{self.stage_name}] Worker '{self.worker_id}' COMPLETED | processed={self.processed_count} | errors={self.error_count} | avg_time={stats['avg_processing_time']:.2f}s")
            return stats
            
        finally:
            # æ¸…ç†äº‹ä»¶å¾ªç¯
            if self.loop is not None:
                self.loop.close()
                logger.info(f"Worker {self.worker_id} event loop closed")


class StreamingPipelineOrchestrator:
    """æµå¼Pipelineç¼–æ’å™¨ - æ”¯æŒåƒä¸‡çº§æ•°æ®å¤„ç†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.pipeline_config = PipelineConfig(**config.get('pipeline', {}))
        self.data_config = config['data']
        
        # Pipelineç»„ä»¶
        self.producer = None
        self.stage_workers: Dict[str, List] = {}  # stage_name -> [workers]
        self.stage_queues: Dict[str, Queue] = {}  # stage_name -> queue
        self.dead_letter_queue = None
        
        # ç›‘æ§
        self.monitoring_system = None
        self.stats = defaultdict(int)
        self.start_time = None
        
        # åˆå§‹åŒ–Ray
        if not ray.is_initialized():
            ray.init(
                object_store_memory=self.pipeline_config.object_store_memory,
                ignore_reinit_error=True
            )
            logger.info("Ray initialized")
    
    def setup_multi_stage_pipeline(self, stages_config: List[Dict[str, Any]]) -> None:
        """è®¾ç½®å¤šé˜¶æ®µæµæ°´çº¿
        
        Args:
            stages_config: List of stage configurations, each containing:
                - type: 'cpu' or 'gpu'
                - class: Stage class to instantiate
                - config: Configuration dictionary for the stage
                - name: Stage name for logging
                - num_workers: Number of workers for this stage (optional)
        """
        logger.info(f"Setting up streaming pipeline with {len(stages_config)} stages")
        
        # åˆ›å»ºç”Ÿäº§è€…
        self.producer = StreamingDataProducer.remote(
            self.data_config,
            self.pipeline_config.batch_size,
            self.pipeline_config.checkpoint_dir
        )
        
        # åˆ›å»ºæ­»ä¿¡é˜Ÿåˆ—
        self.dead_letter_queue = Queue(maxsize=1000)
        
        # åˆ›å»ºå„é˜¶æ®µçš„é˜Ÿåˆ—å’Œworkers
        for stage_idx, stage_config in enumerate(stages_config):
            stage_name = stage_config.get('name', f"stage_{stage_idx}")
            stage_type = stage_config['type']
            stage_class = stage_config['class']
            stage_params = stage_config['config']
            
            # ç¡®å®šworkeræ•°é‡
            if 'num_workers' in stage_config:
                num_workers = stage_config['num_workers']
            else:
                num_workers = (self.pipeline_config.num_cpu_workers 
                              if stage_type == 'cpu' 
                              else self.pipeline_config.num_gpu_workers)
            
            # åˆ›å»ºé˜¶æ®µé˜Ÿåˆ—ï¼ˆèƒŒå‹æ§åˆ¶ï¼‰
            queue_size = self.pipeline_config.queue_max_size
            self.stage_queues[stage_name] = Queue(maxsize=queue_size)
            
            # åˆ›å»ºworkers
            workers = []
            resource_config = (self.pipeline_config.cpu_worker_resources 
                             if stage_type == 'cpu' 
                             else self.pipeline_config.gpu_worker_resources)
            
            if resource_config is None:
                resource_config = {"num_cpus": 1} if stage_type == 'cpu' else {"num_cpus": 1, "num_gpus": 1}
            
            for worker_idx in range(num_workers):
                worker = StreamingPipelineWorker.options(**resource_config).remote(
                    f"{stage_name}_worker_{worker_idx}",
                    stage_name,
                    stage_class,
                    stage_params,
                    self.pipeline_config.max_retries
                )
                workers.append(worker)
            
            self.stage_workers[stage_name] = workers
            
            logger.info(f"Setup stage '{stage_name}': {num_workers} {stage_type} workers, queue_size={queue_size}")
        
        # è¾“å‡ºpipelineæ‹“æ‰‘ç»“æ„
        stage_names = list(self.stage_queues.keys())
        topology = " -> ".join(stage_names)
        logger.info(f"[PIPELINE] Topology: PRODUCER -> {topology} -> RESULTS")
        logger.info(f"[PIPELINE] Total workers: {sum(len(w) for w in self.stage_workers.values())}")
        
        logger.info("Streaming pipeline setup completed")
    
    def run(self,
            max_batches: Optional[int] = None,
            progress_callback: Optional[Callable] = None,
            monitoring_system: Optional[Any] = None) -> Dict[str, Any]:
        """è¿è¡Œæµå¼Pipeline
        
        Returns:
            Pipeline execution statistics
        """
        self.start_time = time.time()
        self.monitoring_system = monitoring_system
        
        if not self.producer:
            raise ValueError("Pipeline not setup. Call setup_multi_stage_pipeline() first.")
        
        logger.info("Starting streaming pipeline execution")
        
        try:
            # è·å–æ‰€æœ‰é˜¶æ®µåç§°ï¼ˆæŒ‰é¡ºåºï¼‰
            stage_names = list(self.stage_queues.keys())
            
            # è®¡ç®—ç¬¬ä¸€é˜¶æ®µçš„workeræ•°é‡
            first_stage_worker_count = len(self.stage_workers[stage_names[0]])
            
            # å¯åŠ¨ç”Ÿäº§è€…ï¼ˆå¼‚æ­¥ï¼‰- ä¼ å…¥ç¬¬ä¸€é˜¶æ®µçš„workeræ•°é‡
            producer_queue = self.stage_queues[stage_names[0]]
            producer_task = self.producer.stream_batches.remote(
                producer_queue,
                max_batches,
                first_stage_worker_count  # å‘é€å¯¹åº”æ•°é‡çš„ç»“æŸä¿¡å·
            )
            logger.info(f"[PIPELINE] Producer started, will send {first_stage_worker_count} END_OF_STREAM signals to {stage_names[0]}")
            
            # å¯åŠ¨æ‰€æœ‰é˜¶æ®µçš„workers
            worker_tasks = []
            for stage_idx, stage_name in enumerate(stage_names):
                input_queue = self.stage_queues[stage_name]
                
                # ç¡®å®šè¾“å‡ºé˜Ÿåˆ—å’Œä¸‹æ¸¸workeræ•°é‡
                if stage_idx < len(stage_names) - 1:
                    output_queue = self.stage_queues[stage_names[stage_idx + 1]]
                    next_stage_name = stage_names[stage_idx + 1]
                    num_downstream_workers = len(self.stage_workers[next_stage_name])
                else:
                    # æœ€åä¸€ä¸ªé˜¶æ®µï¼Œè¾“å‡ºåˆ°ç»“æœé˜Ÿåˆ—
                    output_queue = Queue(maxsize=self.pipeline_config.queue_max_size)
                    self.stage_queues['results'] = output_queue
                    num_downstream_workers = 1  # ç»“æœé˜Ÿåˆ—åªéœ€è¦1ä¸ªç»“æŸä¿¡å·
                
                # åˆ›å»ºç»ˆæ­¢å±éšœ (Termination Barrier)
                # upstream_count = å½“å‰stage workeræ•°é‡
                # downstream_count = ä¸‹æ¸¸éœ€è¦æ¥æ”¶çš„ä¿¡å·æ•°é‡
                current_stage_worker_count = len(self.stage_workers[stage_name])
                barrier = TerminationBarrier.remote(
                    current_stage_worker_count,
                    num_downstream_workers,
                    output_queue,
                    stage_name
                )
                
                # å¯åŠ¨è¯¥é˜¶æ®µçš„æ‰€æœ‰workers
                for worker in self.stage_workers[stage_name]:
                    task = worker.process_stream.remote(
                        input_queue,
                        output_queue,
                        self.dead_letter_queue,
                        num_downstream_workers,
                        barrier  # ä¼ å…¥å±éšœ
                    )
                    worker_tasks.append((stage_name, task))
            
            # å¯åŠ¨ç»“æœæ”¶é›†çº¿ç¨‹ï¼ˆé¿å…æ­»é”çš„å…³é”®ï¼šå¿…é¡»åœ¨ç­‰å¾…workerä¹‹å‰å¼€å§‹æ¶ˆè´¹ç»“æœï¼‰
            results = []
            collection_thread = threading.Thread(
                target=self._collect_results,
                args=(results,),
                daemon=True
            )
            collection_thread.start()
            logger.info("[PIPELINE] Result collection thread started")

            # ç›‘æ§è¿›åº¦
            progress_thread = threading.Thread(
                target=self._monitor_progress,
                args=(progress_callback,),
                daemon=True
            )
            progress_thread.start()
            
            # ç­‰å¾…æ‰€æœ‰workerså®Œæˆ
            logger.info("Waiting for pipeline workers to complete...")
            
            worker_stats = defaultdict(list)
            for stage_name, task in worker_tasks:
                try:
                    stats = ray.get(task, timeout=self.pipeline_config.worker_timeout)
                    worker_stats[stage_name].append(stats)
                except Exception as e:
                    import traceback
                    logger.error(f"Worker task failed in stage '{stage_name}': {e}")
                    logger.error(f"Stage '{stage_name}' traceback:\n{traceback.format_exc()}")
            
            # ç­‰å¾…ç”Ÿäº§è€…å®Œæˆ
            ray.get(producer_task)
            
            # ç­‰å¾…ç»“æœæ”¶é›†å®Œæˆ
            collection_thread.join(timeout=60)
            if collection_thread.is_alive():
                logger.warning("[PIPELINE] Result collection thread did not finish cleanly")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            execution_stats = self._compute_stats(worker_stats, results)
            
            logger.info("=" * 60)
            logger.info("Pipeline Execution Completed")
            logger.info("=" * 60)
            logger.info(f"Total Duration:   {execution_stats['total_duration']:.2f}s")
            logger.info(f"Total Batches:    {execution_stats['total_batches']}")
            logger.info(f"Total Items:      {execution_stats['total_items']}")
            logger.info(f"Success Rate:     {execution_stats['success_rate']:.2%}")
            logger.info(f"Throughput:       {execution_stats['throughput']:.2f} items/s")
            logger.info(f"Dead Letter:      {execution_stats['dead_letter_count']}")
            logger.info("-" * 60)
            logger.info("Stage Statistics:")
            for stage, s in execution_stats['stage_stats'].items():
                logger.info(f"  {stage:<20} | Processed: {s['processed']:<8} | Errors: {s['errors']:<6} | Avg Time: {s['avg_processing_time']:.3f}s")
            logger.info("=" * 60)
            
            return execution_stats
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}")
            raise
        finally:
            self._cleanup()
            
    def _collect_results(self, results_list: List[BatchData]) -> None:
        """æ”¶é›†Pipelineç»“æœï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        result_queue = self.stage_queues.get('results')
        if not result_queue:
            return

        logger.info("[PIPELINE] Result collector started")
        while True:
            try:
                # é˜»å¡ç›´åˆ°æœ‰æ•°æ®ï¼Œè¶…æ—¶ç”¨äºæ£€æŸ¥é€€å‡ºæ¡ä»¶
                batch = result_queue.get(block=True, timeout=5)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºPipelineSignalç»“æŸä¿¡å·
                if isinstance(batch, PipelineSignal):
                    if batch.signal_type == END_OF_STREAM:
                        logger.info(f"[PIPELINE] Received END_OF_STREAM signal in results: {batch}")
                        # åªæœ‰æ”¶åˆ°æ˜ç¡®çš„EOSä¿¡å·æ‰é€€å‡º
                        break
                elif batch is None:
                    # å…¼å®¹æ—§ç‰ˆæœ¬
                    logger.info("[PIPELINE] Received None signal in results")
                    break
                else:
                    # æ­£å¸¸æ•°æ®
                    results_list.append(batch)
                    
            except Empty:
                # é˜Ÿåˆ—æš‚æ—¶ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"[PIPELINE] Error collecting results: {e}")
                break
        
        logger.info(f"[PIPELINE] Collected {len(results_list)} result batches")

    def _monitor_progress(self, progress_callback: Optional[Callable]) -> None:
        """ç›‘æ§Pipelineè¿›åº¦"""
        last_update = time.time()
        update_interval = 5.0  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡
        
        while True:
            try:
                current_time = time.time()
                if current_time - last_update < update_interval:
                    time.sleep(1)
                    continue
                
                # æ”¶é›†é˜Ÿåˆ—çŠ¶æ€
                queue_stats = {}
                queue_summary = []
                for stage_name, queue in self.stage_queues.items():
                    size = queue.qsize()
                    maxsize = queue.maxsize
                    usage_pct = (size / maxsize * 100) if maxsize > 0 else 0
                    queue_stats[stage_name] = {
                        'size': size,
                        'maxsize': maxsize,
                        'usage_pct': usage_pct
                    }
                    # åˆ›å»ºçŠ¶æ€æŒ‡ç¤ºç¬¦
                    if usage_pct > 80:
                        indicator = 'ğŸ”´'  # é˜Ÿåˆ—æ¥è¿‘æ»¡
                    elif usage_pct > 50:
                        indicator = 'ğŸŸ¡'  # ä¸­ç­‰è´Ÿè½½
                    elif usage_pct > 10:
                        indicator = 'ğŸŸ¢'  # æ­£å¸¸
                    else:
                        indicator = 'âšª'  # ç©ºé—²
                    queue_summary.append(f"{stage_name}:{size}/{maxsize}({usage_pct:.0f}%){indicator}")
                
                # è¾“å‡ºé˜Ÿåˆ—çŠ¶æ€æ±‡æ€»
                elapsed = current_time - self.start_time if self.start_time else 0
                logger.info(f"[PIPELINE] Elapsed: {elapsed:.1f}s | Queue Status: {' | '.join(queue_summary)}")
                
                # æ£€æŸ¥æ½œåœ¨ç“¶é¢ˆ
                for stage_name, stats in queue_stats.items():
                    if stats['usage_pct'] > 90:
                        logger.warning(f"[PIPELINE] BACKPRESSURE: Queue '{stage_name}' is {stats['usage_pct']:.0f}% full!")
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(0, queue_stats)
                
                # é›†æˆç›‘æ§ç³»ç»Ÿ
                if self.monitoring_system:
                    for stage_name, stats in queue_stats.items():
                        self.monitoring_system.metrics_collector.update_queue_size(
                            stage_name, stats['size']
                        )
                
                last_update = current_time
                
            except Exception as e:
                logger.error(f"Error in progress monitoring: {e}")
                break
    
    def _compute_stats(self,
                      worker_stats: Dict[str, List[Dict]],
                      results: List[BatchData]) -> Dict[str, Any]:
        """è®¡ç®—Pipelineç»Ÿè®¡ä¿¡æ¯"""
        total_duration = time.time() - self.start_time
        
        # ç»Ÿè®¡æ¯ä¸ªé˜¶æ®µ
        stage_stats = {}
        for stage_name, stats_list in worker_stats.items():
            total_processed = sum(s['processed_count'] for s in stats_list)
            total_errors = sum(s['error_count'] for s in stats_list)
            avg_time = sum(s['avg_processing_time'] for s in stats_list) / len(stats_list) if stats_list else 0
            
            stage_stats[stage_name] = {
                'processed': total_processed,
                'errors': total_errors,
                'avg_processing_time': avg_time,
                'num_workers': len(stats_list)
            }
        
        # ç»Ÿè®¡ç»“æœ
        total_items = sum(len(batch.items) for batch in results)
        successful_items = sum(
            len([item for item in batch.items if not item.metadata.get('error')])
            for batch in results
        )
        
        # æ­»ä¿¡é˜Ÿåˆ—
        dead_letter_count = self.dead_letter_queue.qsize()
        
        return {
            'total_duration': total_duration,
            'total_batches': len(results),
            'total_items': total_items,
            'successful_items': successful_items,
            'failed_items': total_items - successful_items,
            'success_rate': successful_items / total_items if total_items > 0 else 0,
            'throughput': total_items / total_duration if total_duration > 0 else 0,
            'dead_letter_count': dead_letter_count,
            'stage_stats': stage_stats
        }
    
    def _cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up pipeline resources...")
        
        # æ¸…ç©ºé˜Ÿåˆ—
        for queue in self.stage_queues.values():
            while not queue.empty():
                try:
                    queue.get_nowait()
                except Empty:
                    break
    
    def get_checkpoint_status(self) -> Dict[str, Any]:
        """è·å–æ£€æŸ¥ç‚¹çŠ¶æ€"""
        checkpoint_dir = Path(self.pipeline_config.checkpoint_dir)
        
        checkpoint_files = list(checkpoint_dir.glob("*.pkl"))
        
        status = {
            'checkpoint_dir': str(checkpoint_dir),
            'num_checkpoints': len(checkpoint_files),
            'checkpoints': []
        }
        
        for ckpt_file in checkpoint_files:
            status['checkpoints'].append({
                'name': ckpt_file.name,
                'size': ckpt_file.stat().st_size,
                'modified': ckpt_file.stat().st_mtime
            })
        
        return status
    
    def cleanup(self) -> None:
        """æ¸…ç†Pipelineèµ„æº"""
        self._cleanup()


# å‘åå…¼å®¹çš„åŒ…è£…å™¨
class PipelineOrchestrator:
    """é«˜çº§Pipelineç¼–æ’å™¨ï¼ˆå‘åå…¼å®¹ï¼‰"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.orchestrator = StreamingPipelineOrchestrator(config)
    
    def setup_multi_stage_pipeline(self, stages_config: List[Dict[str, Any]]) -> None:
        """è®¾ç½®å¤šé˜¶æ®µPipeline"""
        self.orchestrator.setup_multi_stage_pipeline(stages_config)
    
    def run(self,
            max_batches: Optional[int] = None,
            progress_callback: Optional[Callable] = None) -> List[BatchData]:
        """è¿è¡ŒPipelineï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
        stats = self.orchestrator.run(max_batches, progress_callback)
        
        # è¿”å›ç©ºåˆ—è¡¨ä»¥ä¿æŒå‘åå…¼å®¹ï¼ˆå®é™…ç»“æœé€šè¿‡ResultWriterå¤„ç†ï¼‰
        return []
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.orchestrator.get_checkpoint_status()
    
    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        self.orchestrator.cleanup()